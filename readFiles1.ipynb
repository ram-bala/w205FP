{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #Step 1:  Read in the flat files, and create parquet format tables.\n",
      "    from pyspark import SparkContext\n",
      "    from pyspark.sql import SQLContext\n",
      "    from pyspark.sql.types import *\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import subprocess\n",
      "def run_cmd(args_list):\n",
      "        \"\"\"\n",
      "        run linux commands\n",
      "        \"\"\"\n",
      "        # import subprocess\n",
      "        print('Running system command: {0}'.format(' '.join(args_list)))\n",
      "        proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "        s_output, s_err = proc.communicate()\n",
      "        s_return =  proc.returncode\n",
      "        return s_return, s_output, s_err \n",
      "def hdfs_exists(hdfspath):\n",
      "    cmd = ['hdfs', 'dfs', '-test', '-e', hdfspath]\n",
      "    ret, out, err = run_cmd(cmd)\n",
      "    print(ret, out, err)\n",
      "    return ret\n",
      "def hdfs_deletedir(hdfspath):\n",
      "    cmd= ['hdfs', 'dfs', '-rm -R', hdfspath]\n",
      "    ret,out,err = run_cmd(cmd)\n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "print hdfs_exists('/user/w205/FPParquetFiles/members')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/members\n",
        "(1, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    sqlContext = SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read members file (read the small files first then change to large ones later)\n",
      "#define the schema\n",
      "#members_v3.csv:msno,city,bd,gender,registered_via,registration_init_time\n",
      "#write a function that takes a fileloc, schema, parquet file loc and does the following\n",
      "# reads the csv file, creates parquet and stores it in hdfs\n",
      "\n",
      "def createParquetFile(csvHDFSFile, pqHDFSFile):\n",
      "#check to see if it exists, if not proceed, if exists then delete the dir before creating parquet file\n",
      "    ret_del = 1\n",
      "    if hdfs_exists(pqHDFSFile)==0:\n",
      "        print \"deleting\", pqHDFSFile\n",
      "        ret_del = hdfs_deletedir(pqHDFSFile)\n",
      "    else:\n",
      "        ret_del = 0\n",
      "    if ret_del == 0:\n",
      "        df = sqlContext.read.format(\"com.databricks.spark.csv\").option \\\n",
      "             (\"header\",\"true\").option(\"delimiter\",\",\").load(csvHDFSFile)\n",
      "        df.write.parquet(''.join(['hdfs://',pqHDFSFile]))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LargecsvPrefix = \"hdfs:///user/w205/FP/\"\n",
      "SmallcsvPrefix = \"hdfs:///user/w205/FPSmall/\"\n",
      "pqPrefix = \"/user/w205/FPParquetFiles/\"\n",
      "\n",
      "pq_file_list = ['members','transactions','user_logs', 'train', 'sample_submission']\n",
      "csv_file_list =[]\n",
      "for f in pq_file_list:\n",
      "    csvHDFSFile = ''.join([SmallcsvPrefix, f])\n",
      "    pqHDFSFile = ''.join([pqPrefix, f])\n",
      "    print csvHDFSFile\n",
      "    print pqHDFSFile\n",
      "    createParquetFile(csvHDFSFile, pqHDFSFile)\n",
      "\n",
      "\n",
      "#readdf = sqlContext.read.parquet(pqHDFSFile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hdfs:///user/w205/FPSmall/members\n",
        "/user/w205/FPParquetFiles/members\n",
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/members\n",
        "(0, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "deleting /user/w205/FPParquetFiles/members\n",
        "Running system command: hdfs dfs -rm -R /user/w205/FPParquetFiles/members\n",
        "hdfs:///user/w205/FPSmall/transactions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/user/w205/FPParquetFiles/transactions\n",
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/transactions\n",
        "(1, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hdfs:///user/w205/FPSmall/user_logs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/user/w205/FPParquetFiles/user_logs\n",
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/user_logs\n",
        "(1, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hdfs:///user/w205/FPSmall/train"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/user/w205/FPParquetFiles/train\n",
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/train\n",
        "(1, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hdfs:///user/w205/FPSmall/sample_submission"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/user/w205/FPParquetFiles/sample_submission\n",
        "Running system command: hdfs dfs -test -e /user/w205/FPParquetFiles/sample_submission\n",
        "(1, '', '')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csvHDFSFile = \"hdfs:///user/w205/FPSmall/members/members_v3.csv\"\n",
      "pqHDFSFile = \"user/w205/FPParquetFiles/members/\"\n",
      "\n",
      "\n",
      "rdd.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[Row(msno=u'Rb9UwLQTrxzBVwCB6+bCcSQWZ9JiNLC9dXtM1oEsZA8=', city=u'1', bd=u'0', gender=u'', registered_via=u'11', registration_init_time=u'20110911'),\n",
        " Row(msno=u'+tJonkh+O1CA796Fm5X60UMOtB6POHAwPjbTRVl/EuU=', city=u'1', bd=u'0', gender=u'', registered_via=u'7', registration_init_time=u'20110914'),\n",
        " Row(msno=u'cV358ssn7a0f7jZOwGNWS07wCKVqxyiImJUX6xcIwKw=', city=u'1', bd=u'0', gender=u'', registered_via=u'11', registration_init_time=u'20110915'),\n",
        " Row(msno=u'9bzDeJP6sQodK73K5CBlJ6fgIQzPeLnRl0p5B77XP+g=', city=u'1', bd=u'0', gender=u'', registered_via=u'11', registration_init_time=u'20110915'),\n",
        " Row(msno=u'WFLY3s7z4EZsieHCt63XrsdtfTEmJ+2PnnKLH5GY4Tk=', city=u'6', bd=u'32', gender=u'female', registered_via=u'9', registration_init_time=u'20110915'),\n",
        " Row(msno=u'yLkV2gbZ4GLFwqTOXLVHz0VGrMYcgBGgKZ3kj9RiYu8=', city=u'4', bd=u'30', gender=u'male', registered_via=u'9', registration_init_time=u'20110916'),\n",
        " Row(msno=u'jNCGK78YkTyId3H3wFavcBLDmz7pfqlvCfUKf4G1Lw4=', city=u'1', bd=u'0', gender=u'', registered_via=u'7', registration_init_time=u'20110916'),\n",
        " Row(msno=u'WH5Jq4mgtfUFXh2yz+HrcTXKS4Oess4k4W3qKolAeb0=', city=u'5', bd=u'34', gender=u'male', registered_via=u'9', registration_init_time=u'20110916'),\n",
        " Row(msno=u'tKmbR4X5VXjHmxERrckawEMZ4znVy1lAQIR1vV5rdNk=', city=u'5', bd=u'19', gender=u'male', registered_via=u'9', registration_init_time=u'20110917'),\n",
        " Row(msno=u'I0yFvqMoNkM8ZNHb617e1RBzIS/YRKemHO7Wj13EtA0=', city=u'13', bd=u'63', gender=u'male', registered_via=u'9', registration_init_time=u'20110918')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plainTextRDD = sc.textFile(\"hdfs:///user/w205/FPSmall/members/members_v3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_members = pycsv.csvToDataFrame(sqlContext, plainTextRDD, parseDate=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_members.show()\n",
      "pdf_members = df_members.toPandas()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------+----+---+------+--------------+----------------------+\n",
        "|                msno|city| bd|gender|registered_via|registration_init_time|\n",
        "+--------------------+----+---+------+--------------+----------------------+\n",
        "|Rb9UwLQTrxzBVwCB6...|   1|  0|  null|            11|              20110911|\n",
        "|+tJonkh+O1CA796Fm...|   1|  0|  null|             7|              20110914|\n",
        "|cV358ssn7a0f7jZOw...|   1|  0|  null|            11|              20110915|\n",
        "|9bzDeJP6sQodK73K5...|   1|  0|  null|            11|              20110915|\n",
        "|WFLY3s7z4EZsieHCt...|   6| 32|female|             9|              20110915|\n",
        "|yLkV2gbZ4GLFwqTOX...|   4| 30|  male|             9|              20110916|\n",
        "|jNCGK78YkTyId3H3w...|   1|  0|  null|             7|              20110916|\n",
        "|WH5Jq4mgtfUFXh2yz...|   5| 34|  male|             9|              20110916|\n",
        "|tKmbR4X5VXjHmxERr...|   5| 19|  male|             9|              20110917|\n",
        "|I0yFvqMoNkM8ZNHb6...|  13| 63|  male|             9|              20110918|\n",
        "|OoDwiKZM+ZGr9P3fR...|   1|  0|  null|             7|              20110918|\n",
        "|dCvvBHlaOAqgkAcv3...|  22| 18|  male|             9|              20110919|\n",
        "|6bra2AiVV8SGlm7R6...|   4| 34|female|             9|              20110919|\n",
        "|4De1jAxNRABoyRBDZ...|   4| 28|female|             9|              20110920|\n",
        "|iOzdu4IHbJxhB5CPp...|  12| 29|female|             9|              20110922|\n",
        "|hZia/3iyvtThD1kv6...|   1|  0|  null|             9|              20110922|\n",
        "|QlDflP89J5KChheo2...|   1|  0|  null|             9|              20110922|\n",
        "|fOpj0ApSXk1NVODvC...|  13| 31|female|             7|              20110923|\n",
        "|GsPgdrXoV6688/0RF...|   1|  0|  null|             7|              20110924|\n",
        "|QHHKGOY5yLrGdR8WY...|   5| 19|female|             9|              20110925|\n",
        "+--------------------+----+---+------+--------------+----------------------+\n",
        "only showing top 20 rows\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdf_members.dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "msno                      object\n",
        "city                      object\n",
        "bd                        object\n",
        "gender                    object\n",
        "registered_via            object\n",
        "registration_init_time    object\n",
        "dtype: object"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "membersdata = sc.textFile(\"hdfs:///user/w205/FP/members/members_v2.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trialdata = sc.textFile(\"file:///home/w205/trial.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyspark.sql as ps\n",
      "import pyspark.sql.context as SqlContext\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named sparksession",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-19-9bc782ff939e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mSqlContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparksession\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mImportError\u001b[0m: No module named sparksession"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlcontext = ps.SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "u'1.5.0'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_membersdata = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs:///user/w205/FP/members/members_v3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'spark' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-07426e119a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_membersdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hdfs:///user/w205/FP/members/members_v3.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "membersdata = \n",
      "\n",
      "sc.textFile(\"hdfs:///user/w205/FP/members/members_v2.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "795090\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}